{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y54ygnTsS9im",
        "outputId": "3387e505-3b8c-4029-92fb-2f5aba971bc2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1j9G0tXWHAE",
        "outputId": "59410575-42fd-4085-ab8a-c7f98e11cc14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EfficientNetB3WithoutFC(\n",
              "  (features): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
              "        )\n",
              "        (5): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
              "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (8): Conv2dNormActivation(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): AdaptiveAvgPool2d(output_size=1)\n",
              "  )\n",
              "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths for saving precomputed features or loading cached features\n",
        "FEATURE_DIR = \"feat_embedding_1_10\"\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "\n",
        "# Transformation pipeline for EfficientNet-B3\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(300),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# EfficientNet-B3 model without the final layer for feature extraction\n",
        "class EfficientNetB3WithoutFC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetB3WithoutFC, self).__init__()\n",
        "        original_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-1])  # Remove FC layer\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Add adaptive pooling for feature extraction\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        return x.view(x.size(0), -1)  # Flatten the output\n",
        "\n",
        "feature_extractor = EfficientNetB3WithoutFC().to(device)\n",
        "\n",
        "# This is to show the model architecture used for feature extraction\n",
        "feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2A8A7aq3WHAH"
      },
      "outputs": [],
      "source": [
        "# Function to extract and cache features in batches\n",
        "\n",
        "def load_or_compute_features(images, dataset_name, transform, model, batch_size=64):\n",
        "    cache_path = os.path.join(FEATURE_DIR, f\"{dataset_name}_features.npy\")\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached features for {dataset_name}...\")\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    dataset = torch.utils.data.TensorDataset(torch.stack([transform(img) for img in images]))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting features for {dataset_name}\"):\n",
        "            batch = batch[0].to(device)  # Access data and move to device\n",
        "            feature_batch = model(batch).cpu().numpy()\n",
        "            features.append(feature_batch)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    np.save(cache_path, features)  # Cache features for future reuse\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "# Define LwP Classifier using Mahalanobis distance\n",
        "class LwPClassifierMahalanobis:\n",
        "    def __init__(self):\n",
        "        self.prototypes = None\n",
        "        self.covariances = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.prototypes = {}\n",
        "        self.covariances = {}\n",
        "\n",
        "        for label in self.classes:\n",
        "            class_features = X[y == label]\n",
        "            self.prototypes[label] = np.mean(class_features, axis=0)\n",
        "\n",
        "            # Compute covariance matrix with shrinkage to handle singularity\n",
        "            cov_matrix = np.cov(class_features, rowvar=False)\n",
        "            cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-6  # Regularization\n",
        "            self.covariances[label] = np.linalg.inv(cov_matrix)  # Inverse covariance matrix\n",
        "\n",
        "    def mahalanobis_distance(self, x, prototype, cov_inv):\n",
        "        delta = x - prototype\n",
        "        return np.sqrt(delta.T @ cov_inv @ delta)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate Mahalanobis distance to each prototype and choose the nearest\n",
        "            distances = {\n",
        "                label: self.mahalanobis_distance(x, self.prototypes[label], self.covariances[label])\n",
        "                for label in self.classes\n",
        "            }\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def update(self, X, y):\n",
        "        # Update prototypes and covariance matrices with new data\n",
        "        for label in self.classes:\n",
        "            class_features = X[y == label]\n",
        "            if label in self.prototypes:\n",
        "                # Combine old and new prototypes and covariances\n",
        "                old_mean = self.prototypes[label]\n",
        "                old_cov_inv = self.covariances[label]\n",
        "                new_mean = np.mean(class_features, axis=0)\n",
        "\n",
        "                # Update mean using weighted average\n",
        "                self.prototypes[label] = 0.85 * old_mean + 0.15 * new_mean\n",
        "\n",
        "                # Update covariance using weighted average\n",
        "                new_cov_matrix = np.cov(class_features, rowvar=False) + np.eye(class_features.shape[1]) * 1e-6\n",
        "                self.covariances[label] = 0.85 * old_cov_inv + 0.15 * np.linalg.inv(new_cov_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uhaPhfpVWHAH"
      },
      "outputs": [],
      "source": [
        "# Update process_datasets_sequentially to consistently use caching\n",
        "def process_datasets_sequentially(train_paths, eval_paths, lwp, scaler):\n",
        "    # print(\"\\nProcessing evaluation dataset 1...\")\n",
        "    eval_data_1 = torch.load(eval_paths[0])\n",
        "    eval_images_1 = eval_data_1['data']\n",
        "    eval_labels_1 = eval_data_1['targets']\n",
        "\n",
        "    # Load or compute features\n",
        "    eval_features_1 = load_or_compute_features(eval_images_1, \"eval_1\", transform, feature_extractor)\n",
        "\n",
        "    # Evaluate accuracy on evaluation dataset 1\n",
        "    eval_predictions_1 = lwp.predict(eval_features_1)\n",
        "    eval_accuracy_1 = accuracy_score(eval_labels_1, eval_predictions_1)\n",
        "    print(f\"Accuracy on evaluation dataset 1: {eval_accuracy_1 * 100:.2f}%\")\n",
        "\n",
        "    eval_accuracies = [eval_accuracy_1]  # Store accuracies for evaluation datasets processed so far\n",
        "\n",
        "    for i, (train_path, eval_path) in enumerate(zip(train_paths[1:], eval_paths[1:]), start=2):\n",
        "        print(f\"\\nProcessing training dataset {i}...\")\n",
        "        train_data = torch.load(train_path)\n",
        "        train_images = train_data['data']\n",
        "\n",
        "        # Load or compute training features\n",
        "        train_features = load_or_compute_features(train_images, f\"train_{i}\", transform, feature_extractor)\n",
        "\n",
        "        # Predict pseudo-labels for the training dataset\n",
        "        pseudo_labels = lwp.predict(train_features)\n",
        "\n",
        "        # Update the LwP Classifier\n",
        "        lwp.update(train_features, pseudo_labels)\n",
        "\n",
        "        # Evaluate the updated LwP on all previous evaluation datasets\n",
        "        print(f\"\\nEvaluating updated classifier on datasets 1 to {i-1}...\")\n",
        "        for j in range(1, i):  # Loop through all previous evaluation datasets\n",
        "            eval_data_prev = torch.load(eval_paths[j - 1])\n",
        "            eval_images_prev = eval_data_prev['data']\n",
        "            eval_labels_prev = eval_data_prev['targets']\n",
        "\n",
        "            eval_features_prev = load_or_compute_features(eval_images_prev, f\"eval_{j}\", transform, feature_extractor)\n",
        "\n",
        "            eval_predictions_prev = lwp.predict(eval_features_prev)\n",
        "            eval_accuracy_prev = accuracy_score(eval_labels_prev, eval_predictions_prev)\n",
        "            print(f\"Accuracy on evaluation dataset {j}: {eval_accuracy_prev * 100:.2f}%\")\n",
        "\n",
        "        # Load and process current evaluation dataset\n",
        "        print(f\"\\nProcessing evaluation dataset {i}...\")\n",
        "        eval_data = torch.load(eval_path)\n",
        "        eval_images = eval_data['data']\n",
        "        eval_labels = eval_data['targets']\n",
        "\n",
        "        eval_features = load_or_compute_features(eval_images, f\"eval_{i}\", transform, feature_extractor)\n",
        "\n",
        "        # Evaluate accuracy on the current evaluation dataset\n",
        "        eval_predictions = lwp.predict(eval_features)\n",
        "        eval_accuracy = accuracy_score(eval_labels, eval_predictions)\n",
        "        print(f\"Accuracy on evaluation dataset {i}: {eval_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sut4tMcWHAI",
        "outputId": "47620b3f-9549-4bcd-981a-b5cf558c312e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cached features for train_1...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 90.96%\n",
            "\n",
            "Processing training dataset 2...\n",
            "Loading cached features for train_2...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 1...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 91.24%\n",
            "\n",
            "Processing evaluation dataset 2...\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 92.72%\n",
            "\n",
            "Processing training dataset 3...\n",
            "Loading cached features for train_3...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 2...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 91.00%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 93.04%\n",
            "\n",
            "Processing evaluation dataset 3...\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.80%\n",
            "\n",
            "Processing training dataset 4...\n",
            "Loading cached features for train_4...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 3...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 90.48%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 92.96%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.76%\n",
            "\n",
            "Processing evaluation dataset 4...\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 91.44%\n",
            "\n",
            "Processing training dataset 5...\n",
            "Loading cached features for train_5...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 4...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 90.28%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 92.84%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.76%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 91.08%\n",
            "\n",
            "Processing evaluation dataset 5...\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 91.24%\n",
            "\n",
            "Processing training dataset 6...\n",
            "Loading cached features for train_6...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 5...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 89.88%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 92.32%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.68%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 91.04%\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 91.28%\n",
            "\n",
            "Processing evaluation dataset 6...\n",
            "Loading cached features for eval_6...\n",
            "Accuracy on evaluation dataset 6: 91.08%\n",
            "\n",
            "Processing training dataset 7...\n",
            "Loading cached features for train_7...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 6...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 89.92%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 92.08%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.40%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 90.96%\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 91.12%\n",
            "Loading cached features for eval_6...\n",
            "Accuracy on evaluation dataset 6: 90.76%\n",
            "\n",
            "Processing evaluation dataset 7...\n",
            "Loading cached features for eval_7...\n",
            "Accuracy on evaluation dataset 7: 90.76%\n",
            "\n",
            "Processing training dataset 8...\n",
            "Loading cached features for train_8...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 7...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 89.48%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 91.80%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 91.16%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 90.92%\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 90.68%\n",
            "Loading cached features for eval_6...\n",
            "Accuracy on evaluation dataset 6: 90.68%\n",
            "Loading cached features for eval_7...\n",
            "Accuracy on evaluation dataset 7: 90.68%\n",
            "\n",
            "Processing evaluation dataset 8...\n",
            "Loading cached features for eval_8...\n",
            "Accuracy on evaluation dataset 8: 90.96%\n",
            "\n",
            "Processing training dataset 9...\n",
            "Loading cached features for train_9...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 8...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 89.00%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 91.32%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 90.72%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 89.96%\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 90.32%\n",
            "Loading cached features for eval_6...\n",
            "Accuracy on evaluation dataset 6: 90.04%\n",
            "Loading cached features for eval_7...\n",
            "Accuracy on evaluation dataset 7: 90.20%\n",
            "Loading cached features for eval_8...\n",
            "Accuracy on evaluation dataset 8: 90.20%\n",
            "\n",
            "Processing evaluation dataset 9...\n",
            "Loading cached features for eval_9...\n",
            "Accuracy on evaluation dataset 9: 89.56%\n",
            "\n",
            "Processing training dataset 10...\n",
            "Loading cached features for train_10...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 9...\n",
            "Loading cached features for eval_1...\n",
            "Accuracy on evaluation dataset 1: 88.72%\n",
            "Loading cached features for eval_2...\n",
            "Accuracy on evaluation dataset 2: 90.80%\n",
            "Loading cached features for eval_3...\n",
            "Accuracy on evaluation dataset 3: 90.36%\n",
            "Loading cached features for eval_4...\n",
            "Accuracy on evaluation dataset 4: 89.60%\n",
            "Loading cached features for eval_5...\n",
            "Accuracy on evaluation dataset 5: 89.76%\n",
            "Loading cached features for eval_6...\n",
            "Accuracy on evaluation dataset 6: 89.52%\n",
            "Loading cached features for eval_7...\n",
            "Accuracy on evaluation dataset 7: 89.40%\n",
            "Loading cached features for eval_8...\n",
            "Accuracy on evaluation dataset 8: 89.72%\n",
            "Loading cached features for eval_9...\n",
            "Accuracy on evaluation dataset 9: 89.28%\n",
            "\n",
            "Processing evaluation dataset 10...\n",
            "Loading cached features for eval_10...\n",
            "Accuracy on evaluation dataset 10: 89.32%\n"
          ]
        }
      ],
      "source": [
        "# Paths for train and eval datasets\n",
        "train_paths = [f\"dataset/part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "eval_paths = [f\"dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Load the first training dataset\n",
        "train_data_1 = torch.load(train_paths[0])\n",
        "D1_images = train_data_1['data']\n",
        "D1_labels = train_data_1['targets']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "D1_features = load_or_compute_features(D1_images, f\"train_1\", transform, feature_extractor)\n",
        "\n",
        "\n",
        "lwp = LwPClassifierMahalanobis()\n",
        "lwp.fit(D1_features, np.array(D1_labels))\n",
        "\n",
        "process_datasets_sequentially(train_paths, eval_paths, lwp, scaler)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
