{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I50JMLsXesSb",
        "outputId": "eef35804-91ae-438c-ede1-c8590f4e85fd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1j9G0tXWHAE",
        "outputId": "4d4a4ffc-0d3a-43fe-e7c1-34a92b8afa27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EfficientNetB3WithoutFC(\n",
              "  (features): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
              "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
              "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
              "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
              "        )\n",
              "        (2): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
              "        )\n",
              "        (3): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
              "        )\n",
              "        (4): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
              "        )\n",
              "        (5): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
              "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
              "        )\n",
              "        (1): MBConv(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
              "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (2): SiLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): SiLU(inplace=True)\n",
              "              (scale_activation): Sigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
              "        )\n",
              "      )\n",
              "      (8): Conv2dNormActivation(\n",
              "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): AdaptiveAvgPool2d(output_size=1)\n",
              "  )\n",
              "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths for saving precomputed features or loading cached features\n",
        "FEATURE_DIR = \"feat_embedding_1_10\"\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "\n",
        "# Transformation pipeline for EfficientNet-B3\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(300),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# EfficientNet-B3 model without the final layer for feature extraction\n",
        "class EfficientNetB3WithoutFC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetB3WithoutFC, self).__init__()\n",
        "        original_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-1])  # Remove FC layer\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Add adaptive pooling for feature extraction\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        return x.view(x.size(0), -1)  # Flatten the output\n",
        "\n",
        "feature_extractor = EfficientNetB3WithoutFC().to(device)\n",
        "feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2A8A7aq3WHAH"
      },
      "outputs": [],
      "source": [
        "# Function to extract and cache features in batches\n",
        "\n",
        "def load_or_compute_features(images, dataset_name, transform, model, batch_size=64):\n",
        "    cache_path = os.path.join(FEATURE_DIR, f\"{dataset_name}_features.npy\")\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached features for {dataset_name}...\")\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    dataset = torch.utils.data.TensorDataset(torch.stack([transform(img) for img in images]))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting features for {dataset_name}\"):\n",
        "            batch = batch[0].to(device)  # Access data and move to device\n",
        "            feature_batch = model(batch).cpu().numpy()\n",
        "            features.append(feature_batch)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    np.save(cache_path, features)  # Cache features for future reuse\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "# Define LwP Classifier using Mahalanobis distance\n",
        "class LwPClassifierMahalanobis:\n",
        "    def __init__(self):\n",
        "        self.prototypes = None\n",
        "        self.covariances = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.prototypes = {}\n",
        "        self.covariances = {}\n",
        "\n",
        "        for label in self.classes:\n",
        "            class_features = X[y == label]\n",
        "            self.prototypes[label] = np.mean(class_features, axis=0)\n",
        "\n",
        "            # Compute covariance matrix with shrinkage to handle singularity\n",
        "            cov_matrix = np.cov(class_features, rowvar=False)\n",
        "            cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-6  # Regularization\n",
        "            self.covariances[label] = np.linalg.inv(cov_matrix)  # Inverse covariance matrix\n",
        "\n",
        "    def mahalanobis_distance(self, x, prototype, cov_inv):\n",
        "        delta = x - prototype\n",
        "        return np.sqrt(delta.T @ cov_inv @ delta)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate Mahalanobis distance to each prototype and choose the nearest\n",
        "            distances = {\n",
        "                label: self.mahalanobis_distance(x, self.prototypes[label], self.covariances[label])\n",
        "                for label in self.classes\n",
        "            }\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def update(self, X, y):\n",
        "        # Update prototypes and covariance matrices with new data\n",
        "        for label in self.classes:\n",
        "            class_features = X[y == label]\n",
        "            if label in self.prototypes:\n",
        "                # Combine old and new prototypes and covariances\n",
        "                old_mean = self.prototypes[label]\n",
        "                old_cov_inv = self.covariances[label]\n",
        "                new_mean = np.mean(class_features, axis=0)\n",
        "\n",
        "                # Update mean using weighted average\n",
        "                self.prototypes[label] = 0.85 * old_mean + 0.15 * new_mean\n",
        "\n",
        "                # Update covariance using weighted average\n",
        "                new_cov_matrix = np.cov(class_features, rowvar=False) + np.eye(class_features.shape[1]) * 1e-6\n",
        "                self.covariances[label] = 0.85 * old_cov_inv + 0.15 * np.linalg.inv(new_cov_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uhaPhfpVWHAH"
      },
      "outputs": [],
      "source": [
        "# Update process_datasets_sequentially to consistently use caching\n",
        "\n",
        "def process_datasets_sequentially(train_paths, eval_paths, lwp):\n",
        "\n",
        "    for i, (train_path, eval_path) in enumerate(zip(train_paths[1:], eval_paths[1:]), start=2):\n",
        "        print(f\"\\nProcessing training dataset {i}...\")\n",
        "        train_data = torch.load(train_path)\n",
        "        train_images = train_data['data']\n",
        "\n",
        "        # Load or compute training features\n",
        "        train_features = load_or_compute_features(train_images, f\"train_{i}\", transform, feature_extractor)\n",
        "\n",
        "        # Predict pseudo-labels for the training dataset\n",
        "        pseudo_labels = lwp.predict(train_features)\n",
        "\n",
        "        # Update the LwP Classifier\n",
        "        lwp.update(train_features, pseudo_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sut4tMcWHAI",
        "outputId": "077f1877-098f-4123-acd9-34015dfd58ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cached features for train_1...\n",
            "\n",
            "Processing training dataset 2...\n",
            "Loading cached features for train_2...\n",
            "\n",
            "Processing training dataset 3...\n",
            "Loading cached features for train_3...\n",
            "\n",
            "Processing training dataset 4...\n",
            "Loading cached features for train_4...\n",
            "\n",
            "Processing training dataset 5...\n",
            "Loading cached features for train_5...\n",
            "\n",
            "Processing training dataset 6...\n",
            "Loading cached features for train_6...\n",
            "\n",
            "Processing training dataset 7...\n",
            "Loading cached features for train_7...\n",
            "\n",
            "Processing training dataset 8...\n",
            "Loading cached features for train_8...\n",
            "\n",
            "Processing training dataset 9...\n",
            "Loading cached features for train_9...\n",
            "\n",
            "Processing training dataset 10...\n",
            "Loading cached features for train_10...\n"
          ]
        }
      ],
      "source": [
        "#Paths for train and eval datasets\n",
        "train_paths = [f\"dataset/part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "eval_paths = [f\"dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Load the first training dataset\n",
        "train_data_1 = torch.load(train_paths[0])\n",
        "D1_images = train_data_1['data']\n",
        "D1_labels = train_data_1['targets']\n",
        "\n",
        "# Extract features for the first dataset\n",
        "D1_features = load_or_compute_features(D1_images, \"train_1\", transform, feature_extractor)\n",
        "\n",
        "# Initialize the LwP classifier with Euclidean distance\n",
        "lwp = LwPClassifierMahalanobis()\n",
        "lwp.fit(D1_features, np.array(D1_labels))\n",
        "\n",
        "# Process datasets sequentially and get the detailed summary\n",
        "detailed_summary_df = process_datasets_sequentially(train_paths, eval_paths, lwp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkgtnULzWHAJ",
        "outputId": "70bc0b01-8beb-43b1-e3ea-a5525256ba95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{np.int64(0): array([ 0.05449521,  0.00056987,  0.07043332, ..., -0.11346348,\n",
            "        0.31664404, -0.10727898], dtype=float32), np.int64(1): array([ 0.08313376,  0.10209099,  0.07645009, ..., -0.01488356,\n",
            "        0.0746142 ,  0.02628328], dtype=float32), np.int64(2): array([ 0.03387743, -0.01180812,  0.0579953 , ..., -0.03884554,\n",
            "        0.14813577,  0.1531308 ], dtype=float32), np.int64(3): array([0.11995536, 0.04611782, 0.04923675, ..., 0.01600114, 0.1440477 ,\n",
            "       0.01228173], dtype=float32), np.int64(4): array([ 0.09725423, -0.03132486,  0.02224294, ...,  0.00401522,\n",
            "        0.1532101 , -0.02752708], dtype=float32), np.int64(5): array([ 0.14929634,  0.06586424,  0.000548  , ..., -0.04714416,\n",
            "        0.02317119,  0.09694136], dtype=float32), np.int64(6): array([-0.0418111 , -0.04189126,  0.08957747, ..., -0.07926816,\n",
            "        0.06450835,  0.10742693], dtype=float32), np.int64(7): array([ 0.20204225,  0.0287745 ,  0.18875253, ..., -0.09134164,\n",
            "        0.10140918, -0.06515303], dtype=float32), np.int64(8): array([ 0.23855871,  0.05547344,  0.05871311, ..., -0.11794676,\n",
            "        0.07304671, -0.09505259], dtype=float32), np.int64(9): array([ 0.18629675,  0.09983298,  0.12024343, ..., -0.03835186,\n",
            "       -0.00294582, -0.03716618], dtype=float32)}\n"
          ]
        }
      ],
      "source": [
        "print(lwp.prototypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LwPClassifierEuclidean:\n",
        "    def __init__(self, prototypes):\n",
        "        self.prototypes = prototypes  # Initialize with prototypes from \"mean.pkl\"\n",
        "        self.classes = list(prototypes.keys()) if prototypes else []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.prototypes = {}\n",
        "\n",
        "        for label in self.classes:\n",
        "            class_features = X[y == label]\n",
        "            self.prototypes[label] = np.mean(class_features, axis=0)\n",
        "\n",
        "    def euclidean_distance(self, x, prototype):\n",
        "        return np.linalg.norm(x - prototype)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Calculate Euclidean distance to each prototype and choose the nearest\n",
        "            distances = {label: self.euclidean_distance(x, self.prototypes[label]) for label in self.classes}\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def update(self, X, y):\n",
        "        for label in np.unique(y):\n",
        "            class_features = X[y == label]\n",
        "            if label in self.prototypes:\n",
        "                # Combine old and new prototypes\n",
        "                old_mean = self.prototypes[label]\n",
        "                new_mean = np.mean(class_features, axis=0)\n",
        "\n",
        "                # Weighted update for prototype\n",
        "                self.prototypes[label] = 0.85 * old_mean + 0.15 * new_mean\n",
        "            else:\n",
        "                # Add a new class prototype if not previously seen\n",
        "                self.prototypes[label] = np.mean(class_features, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5Jr4TIZLm20l"
      },
      "outputs": [],
      "source": [
        "# Updated function to handle separate feature directory for Task 2\n",
        "def load_or_compute_features_task2(images, dataset_name, transform, model, feature_dir, batch_size=32):\n",
        "    cache_path = os.path.join(feature_dir, f\"{dataset_name}_features.npy\")\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached features for {dataset_name} from Task 2 directory...\")\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    dataset = torch.utils.data.TensorDataset(torch.stack([transform(img) for img in images]))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting features for {dataset_name}\"):\n",
        "            batch = batch[0].to(device)  # Access data and move to device\n",
        "            feature_batch = model(batch).cpu().numpy()\n",
        "            features.append(feature_batch)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    np.save(cache_path, features)  # Cache features for future reuse\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdtKeGVirFQK",
        "outputId": "bc6d27bf-fc69-414a-fbf0-17447b2f454e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing training dataset D11...\n",
            "Loading cached features for train_1 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 11...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 86.80%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 88.56%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 88.88%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 88.52%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 88.08%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 88.24%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 87.84%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 88.12%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 87.08%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 87.88%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 72.44%\n",
            "\n",
            "Processing training dataset D12...\n",
            "Loading cached features for train_2 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 12...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 85.88%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 87.84%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 88.68%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 87.88%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 87.44%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 87.84%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 87.40%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 87.88%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 86.48%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 87.60%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 71.96%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 54.28%\n",
            "\n",
            "Processing training dataset D13...\n",
            "Loading cached features for train_3 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 13...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 85.48%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 87.48%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 88.40%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 87.72%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 87.28%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 87.44%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 87.00%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 87.56%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 86.08%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 87.16%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 71.72%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 54.20%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 79.64%\n",
            "\n",
            "Processing training dataset D14...\n",
            "Loading cached features for train_4 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 14...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 85.44%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 87.20%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 88.00%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 87.68%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 87.12%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 87.20%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 86.88%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 87.40%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 85.68%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 86.84%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 71.56%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 53.80%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 79.32%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 85.60%\n",
            "\n",
            "Processing training dataset D15...\n",
            "Loading cached features for train_5 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 15...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 85.12%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 87.20%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 87.64%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 87.52%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 86.96%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 87.28%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 86.68%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 87.08%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 85.36%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 86.68%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 70.96%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 53.52%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 79.24%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 85.52%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 87.00%\n",
            "\n",
            "Processing training dataset D16...\n",
            "Loading cached features for train_6 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 16...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 84.96%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 86.96%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 87.36%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 87.08%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 86.40%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 87.00%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 86.40%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 86.72%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 85.24%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 86.24%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 70.68%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 53.12%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 78.80%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 84.72%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 86.60%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D16: 74.04%\n",
            "\n",
            "Processing training dataset D17...\n",
            "Loading cached features for train_7 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 17...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 84.80%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 86.60%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 87.04%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 86.92%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 86.20%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 86.76%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 86.16%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 86.36%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 84.96%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 86.08%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 70.60%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 52.60%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 78.48%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 84.44%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 86.08%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D16: 73.84%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D17: 79.36%\n",
            "\n",
            "Processing training dataset D18...\n",
            "Loading cached features for train_8 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 18...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 84.40%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 86.32%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 86.76%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 86.56%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 86.04%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 86.44%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 85.80%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 86.16%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 84.60%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 86.00%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 70.60%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 52.60%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 78.40%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 84.44%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 85.80%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D16: 73.44%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D17: 79.16%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D18: 76.48%\n",
            "\n",
            "Processing training dataset D19...\n",
            "Loading cached features for train_9 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 19...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 84.44%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 86.12%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 86.52%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 86.72%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 85.48%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 86.00%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 85.36%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 86.20%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 84.16%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 85.44%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 70.44%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 52.40%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 78.16%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 84.28%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 85.44%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D16: 73.36%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D17: 78.52%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D18: 76.28%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D19: 66.08%\n",
            "\n",
            "Processing training dataset D20...\n",
            "Loading cached features for train_10 from Task 2 directory...\n",
            "\n",
            "Evaluating updated classifier on datasets 1 to 20...\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D1: 84.40%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D2: 86.04%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D3: 86.64%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D4: 86.48%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D5: 85.56%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D6: 85.92%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D7: 85.40%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D8: 86.08%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D9: 84.24%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D10: 85.40%\n",
            "Loading cached features for eval_1 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D11: 69.76%\n",
            "Loading cached features for eval_2 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D12: 52.36%\n",
            "Loading cached features for eval_3 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D13: 77.84%\n",
            "Loading cached features for eval_4 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D14: 84.04%\n",
            "Loading cached features for eval_5 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D15: 85.20%\n",
            "Loading cached features for eval_6 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D16: 73.00%\n",
            "Loading cached features for eval_7 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D17: 78.28%\n",
            "Loading cached features for eval_8 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D18: 76.36%\n",
            "Loading cached features for eval_9 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D19: 65.80%\n",
            "Loading cached features for eval_10 from Task 2 directory...\n",
            "Accuracy on evaluation dataset D20: 83.80%\n",
            "\n",
            "Detailed Summary of Task 2 Evaluation Accuracies:\n",
            "                   Eval Dataset 1  Eval Dataset 2  Eval Dataset 3  \\\n",
            "After Training 11           86.80           88.56           88.88   \n",
            "After Training 12           85.88           87.84           88.68   \n",
            "After Training 13           85.48           87.48           88.40   \n",
            "After Training 14           85.44           87.20           88.00   \n",
            "After Training 15           85.12           87.20           87.64   \n",
            "After Training 16           84.96           86.96           87.36   \n",
            "After Training 17           84.80           86.60           87.04   \n",
            "After Training 18           84.40           86.32           86.76   \n",
            "After Training 19           84.44           86.12           86.52   \n",
            "After Training 20           84.40           86.04           86.64   \n",
            "\n",
            "                   Eval Dataset 4  Eval Dataset 5  Eval Dataset 6  \\\n",
            "After Training 11           88.52           88.08           88.24   \n",
            "After Training 12           87.88           87.44           87.84   \n",
            "After Training 13           87.72           87.28           87.44   \n",
            "After Training 14           87.68           87.12           87.20   \n",
            "After Training 15           87.52           86.96           87.28   \n",
            "After Training 16           87.08           86.40           87.00   \n",
            "After Training 17           86.92           86.20           86.76   \n",
            "After Training 18           86.56           86.04           86.44   \n",
            "After Training 19           86.72           85.48           86.00   \n",
            "After Training 20           86.48           85.56           85.92   \n",
            "\n",
            "                   Eval Dataset 7  Eval Dataset 8  Eval Dataset 9  \\\n",
            "After Training 11           87.84           88.12           87.08   \n",
            "After Training 12           87.40           87.88           86.48   \n",
            "After Training 13           87.00           87.56           86.08   \n",
            "After Training 14           86.88           87.40           85.68   \n",
            "After Training 15           86.68           87.08           85.36   \n",
            "After Training 16           86.40           86.72           85.24   \n",
            "After Training 17           86.16           86.36           84.96   \n",
            "After Training 18           85.80           86.16           84.60   \n",
            "After Training 19           85.36           86.20           84.16   \n",
            "After Training 20           85.40           86.08           84.24   \n",
            "\n",
            "                   Eval Dataset 10  Eval Dataset 11  Eval Dataset 12  \\\n",
            "After Training 11            87.88            72.44              NaN   \n",
            "After Training 12            87.60            71.96            54.28   \n",
            "After Training 13            87.16            71.72            54.20   \n",
            "After Training 14            86.84            71.56            53.80   \n",
            "After Training 15            86.68            70.96            53.52   \n",
            "After Training 16            86.24            70.68            53.12   \n",
            "After Training 17            86.08            70.60            52.60   \n",
            "After Training 18            86.00            70.60            52.60   \n",
            "After Training 19            85.44            70.44            52.40   \n",
            "After Training 20            85.40            69.76            52.36   \n",
            "\n",
            "                   Eval Dataset 13  Eval Dataset 14  Eval Dataset 15  \\\n",
            "After Training 11              NaN              NaN              NaN   \n",
            "After Training 12              NaN              NaN              NaN   \n",
            "After Training 13            79.64              NaN              NaN   \n",
            "After Training 14            79.32            85.60              NaN   \n",
            "After Training 15            79.24            85.52            87.00   \n",
            "After Training 16            78.80            84.72            86.60   \n",
            "After Training 17            78.48            84.44            86.08   \n",
            "After Training 18            78.40            84.44            85.80   \n",
            "After Training 19            78.16            84.28            85.44   \n",
            "After Training 20            77.84            84.04            85.20   \n",
            "\n",
            "                   Eval Dataset 16  Eval Dataset 17  Eval Dataset 18  \\\n",
            "After Training 11              NaN              NaN              NaN   \n",
            "After Training 12              NaN              NaN              NaN   \n",
            "After Training 13              NaN              NaN              NaN   \n",
            "After Training 14              NaN              NaN              NaN   \n",
            "After Training 15              NaN              NaN              NaN   \n",
            "After Training 16            74.04              NaN              NaN   \n",
            "After Training 17            73.84            79.36              NaN   \n",
            "After Training 18            73.44            79.16            76.48   \n",
            "After Training 19            73.36            78.52            76.28   \n",
            "After Training 20            73.00            78.28            76.36   \n",
            "\n",
            "                   Eval Dataset 19  Eval Dataset 20  \n",
            "After Training 11              NaN              NaN  \n",
            "After Training 12              NaN              NaN  \n",
            "After Training 13              NaN              NaN  \n",
            "After Training 14              NaN              NaN  \n",
            "After Training 15              NaN              NaN  \n",
            "After Training 16              NaN              NaN  \n",
            "After Training 17              NaN              NaN  \n",
            "After Training 18              NaN              NaN  \n",
            "After Training 19            66.08              NaN  \n",
            "After Training 20            65.80             83.8  \n"
          ]
        }
      ],
      "source": [
        "# Function to process datasets for Task 2 with correct paths and adjusted dataset naming\n",
        "def process_task2_with_comprehensive_evaluation(train_paths_task2, eval_paths_task1, eval_paths_task2, lwp, feature_dir_task2):\n",
        "    summary = []  # Store evaluation accuracies for all datasets from 1 to i\n",
        "\n",
        "    # Combine Task 1 and Task 2 evaluation paths\n",
        "    eval_paths = eval_paths_task1 + eval_paths_task2\n",
        "\n",
        "    for i, train_path in enumerate(train_paths_task2, start=1):  # Start from 11th dataset for part_two_dataset\n",
        "        print(f\"\\nProcessing training dataset D{i+10}...\")\n",
        "        train_data = torch.load(train_path)\n",
        "        train_images = train_data['data']\n",
        "\n",
        "        # Load or compute features for the training dataset using Task 2 directory\n",
        "        train_features = load_or_compute_features_task2(train_images, f\"train_{i}\", transform, feature_extractor, feature_dir_task2)\n",
        "\n",
        "        # Predict pseudo-labels using the LwP classifier\n",
        "        pseudo_labels = lwp.predict(train_features)\n",
        "\n",
        "        # Update the LwP classifier with the pseudo-labeled data\n",
        "        lwp.update(train_features, pseudo_labels)\n",
        "\n",
        "        # Evaluate on all evaluation datasets from 1 to i\n",
        "        accuracies_for_current_training = []\n",
        "        print(f\"\\nEvaluating updated classifier on datasets 1 to {i+10}...\")\n",
        "        for j in range(1, i + 11):  # Loop through all previous evaluation datasets (1 to i)\n",
        "            eval_data_j = torch.load(eval_paths[j - 1])\n",
        "            eval_images_j = eval_data_j['data']\n",
        "            eval_labels_j = eval_data_j['targets']\n",
        "\n",
        "            if j <= 10:\n",
        "              eval_features_j = load_or_compute_features_task2(eval_images_j, f\"eval_{j}\", transform, feature_extractor, FEATURE_DIR)\n",
        "            else:\n",
        "              eval_features_j = load_or_compute_features_task2(eval_images_j, f\"eval_{j-10}\", transform, feature_extractor, feature_dir_task2)\n",
        "            # Predict labels and calculate accuracy\n",
        "            eval_predictions_j = lwp.predict(eval_features_j)\n",
        "            eval_accuracy_j = accuracy_score(eval_labels_j, eval_predictions_j)\n",
        "            print(f\"Accuracy on evaluation dataset D{j}: {eval_accuracy_j * 100:.2f}%\")\n",
        "            accuracies_for_current_training.append(eval_accuracy_j * 100)\n",
        "\n",
        "        # Append accuracies for this training step to the summary\n",
        "        while len(summary) < (i+10) - 10:  # Ensure summary is aligned for Task 2 (starting at D11)\n",
        "            summary.append([])\n",
        "        summary[(i+10) - 11] = accuracies_for_current_training\n",
        "\n",
        "    # Print final detailed summary for Task 2\n",
        "    print(\"\\nDetailed Summary of Task 2 Evaluation Accuracies:\")\n",
        "    df_summary = pd.DataFrame(summary)\n",
        "    df_summary.index = [f\"After Training {i}\" for i in range(11, 11 + len(summary))]\n",
        "    df_summary.columns = [f\"Eval Dataset {j}\" for j in range(1, df_summary.shape[1] + 1)]\n",
        "    print(df_summary)\n",
        "    return df_summary\n",
        "\n",
        "\n",
        "# Paths for Task 2 datasets\n",
        "FEATURE_DIR_TASK2 = \"feat_embedding_11_20\"  # Separate directory for Task 2 features\n",
        "os.makedirs(FEATURE_DIR_TASK2, exist_ok=True)\n",
        "\n",
        "# Paths for training datasets (Task 2 starts with 11th dataset as 1 in part_two_dataset)\n",
        "train_paths_task2 = [f\"dataset/part_two_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Paths for evaluation datasets\n",
        "eval_paths_task1 = [f\"dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "eval_paths_task2 = [f\"dataset/part_two_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Initialising an lwp euclidean model with the previous prototypes\n",
        "lwp_euclidean = LwPClassifierEuclidean(prototypes=lwp.prototypes)\n",
        "\n",
        "# Process Task 2 datasets with comprehensive evaluation\n",
        "task2_comprehensive_summary_df = process_task2_with_comprehensive_evaluation(train_paths_task2, eval_paths_task1, eval_paths_task2, lwp_euclidean, FEATURE_DIR_TASK2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omdDxtW34JkD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
